{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) #stop displaying tensorflow warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(path):\n",
    "    \"\"\"\n",
    "    Read the Max-Cut graph stored in a .txt/.SPARSE file and returns an adjacency matrix of the Max-Cut graph.\n",
    "    path - filepath of the Max-Cut graph file\n",
    "    adj_matrix - returns adjacency matrix of Max-Cut instance\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        line = f.readline().split()\n",
    "        # get number of nodes and number of undirected edges\n",
    "        N, entries = int(line[0]), int(line[1])\n",
    "        # create a zeros QUBO matrix\n",
    "        adj_matrix = np.zeros(shape=(N,N))\n",
    "        for _ in range(entries):\n",
    "            # extract the node indices and value; fill the adjacency matrix\n",
    "            line = f.readline().split()\n",
    "            node1, node2, value = int(line[0]), int(line[1]), int(line[2])\n",
    "            # fill both Q[i,j] and Q[j,i] as the QUBO matrix is symmetric\n",
    "            adj_matrix[node1-1, node2-1] = value\n",
    "            adj_matrix[node2-1, node1-1] = value\n",
    "\n",
    "        return adj_matrix, N\n",
    "\n",
    "def formulate_qubo(adj_matrix):\n",
    "    \"\"\"\n",
    "    Take an adjacency matrix of a Max-Cut instance as parameter and generate the QUBO matrix of that instance.\n",
    "    adj_matrix - adjacency matrix of Max-Cut instance\n",
    "    qubo - QUBO matrix of Max-Cut instance\n",
    "    \"\"\"    \n",
    "    qubo = adj_matrix.copy()\n",
    "    # get the number of edges of each node by summing the respective row in G\n",
    "    edge_count = np.sum(qubo, axis=1)\n",
    "    # fill the diagonal positions with the corresponding edge_count * -1\n",
    "    np.fill_diagonal(qubo, -edge_count)\n",
    "    return qubo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCA Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for experimental reproducibility\n",
    "seed = 111\n",
    "random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "np.random.seed(seed)  # numpy pseudo-random generator\n",
    "tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "\n",
    "def hamiltonian(samples, Q):\n",
    "    \"\"\"\n",
    "    hamiltonian returns the energies of an array of max-cut solutions.\n",
    "    samples - array of solutions\n",
    "    Q - QUBO matrix\n",
    "    energies - returns the energy of the samples\n",
    "    \"\"\"\n",
    "    xQ = np.dot(samples, Q)\n",
    "    energies = np.einsum('ij,ij->i', xQ, samples)\n",
    "    return energies\n",
    "\n",
    "\"\"\"\n",
    "RNNProbability classes\n",
    "\"\"\"\n",
    "class RNNProbabilityNWS(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[10],scope='RNNProbability', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int\n",
    "                        number of sites\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                        number of units per RNN layer\n",
    "            scope:       str\n",
    "                        the name of the name-space scope\n",
    "            activation:  activation of the RNN cell\n",
    "            seed:        pseudo-random number generator\n",
    "        \"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        self.scope = scope #Label of the RNN probability\n",
    "        self.N = systemsize #Number of sites of the 1D chain\n",
    "\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        # Defining the neural network\n",
    "        # different RNN block being used at every site\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn=[tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell(units[i],activation = activation,name='RNN_{0}{1}'.format(i,n), dtype = tf.float64) for i in range(len(units))]) for n in range(self.N)]\n",
    "                self.dense = [tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='PRNN_dense_{0}'.format(n), dtype = tf.float64) for n in range(self.N)]\n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            numsamples:      int\n",
    "                            number of samples to be produced\n",
    "            inputdim:        int\n",
    "                            hilbert space dimension\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:         a tuple (samples,log-probs)\n",
    "\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                            the samples in integer encoding\n",
    "            log-probs        tf.Tensor of shape (numsamples,)\n",
    "                            the log-probability of each sample\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default(): # Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                \n",
    "                # b = zeros ndarray with shape [numsamples,inputdim] \n",
    "                # b = state of one spin for all the samples, this command above makes all the samples having 1 in the first component and 0 in the second.\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "\n",
    "                probs=[]\n",
    "\n",
    "                # inputs = b ndarray used as a template to create equivalent tensor\n",
    "                # Initial input to feed to the rnn\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "\n",
    "                self.inputdim=inputs.shape[1]   # 2\n",
    "                self.outputdim=self.inputdim    # 2\n",
    "                self.numsamples=inputs.shape[0] # M \n",
    "\n",
    "                rnn_state=self.rnn[0].zero_state(self.numsamples,dtype=tf.float64)  # [numsamples,num_units]\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn[n](inputs, rnn_state)  # elu activation function output of shape [numsamples,num_units] \n",
    "                    output=self.dense[n](rnn_output)    # softmax probability of +1 and -1 spins [numsamples, 2]\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,])   # sampled nth position spin; values are either 0 or 1 [numsamples,]\n",
    "                    probs.append(output) \n",
    "                    samples.append(sample_temp)     # [N, numsamples]\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim, dtype = tf.float64)     # one-hot encoded vector input for subsequent RNN [numsamples, 2]\n",
    "\n",
    "            self.samples=tf.stack(values=samples,axis=1) # [self.N, num_samples] to [num_samples, self.N]: Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples, self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            samples:         tf.Tensor\n",
    "                            a tf.placeholder of shape (number of samples,system-size)\n",
    "                            containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                            dimension of the input space\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                            the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_state=self.rnn[0].zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn[n](inputs, rnn_state)\n",
    "                    output=self.dense[n](rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class RNNProbabilityWS(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[10],scope='RNNProbability', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int\n",
    "                         number of sites\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                         number of units per RNN layer\n",
    "            scope:       str\n",
    "                         the name of the name-space scope\n",
    "            activation:  activation function of the RNN cell\n",
    "            seed:        pseudo-random number generator\n",
    "        \"\"\"\n",
    "        self.graph=tf.Graph()\n",
    "        self.scope=scope #Label of the RNN probability\n",
    "        self.N=systemsize #Number of sites of the 1D chain\n",
    "\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        #Defining the neural network\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell(units[i],activation = activation,name='RNN_{0}'.format(i), dtype = tf.float64) for i in range(len(units))])\n",
    "                self.dense = tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='PRNN_dense', dtype = tf.float64) \n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            numsamples:      int\n",
    "                             number of samples to be produced\n",
    "            inputdim:        int\n",
    "                             hilbert space dimension\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:         a tuple (samples,log-probs)\n",
    "\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                             the samples in integer encoding\n",
    "            log-probs        tf.Tensor of shape (numsamples,)\n",
    "                             the log-probability of each sample\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default(): #Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "                #b = state of one spin for all the samples, this command above makes all the samples having 1 in the first component and 0 in the second.\n",
    "                probs=[]\n",
    "\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "                #Initial input to feed to the rnn\n",
    "\n",
    "                self.inputdim=inputs.shape[1]\n",
    "                self.outputdim=self.inputdim\n",
    "                self.numsamples=inputs.shape[0]\n",
    "\n",
    "                rnn_state=self.rnn.zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn(inputs, rnn_state)\n",
    "                    output=self.dense(rnn_output)\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,])\n",
    "                    probs.append(output)\n",
    "                    samples.append(sample_temp)\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim, dtype = tf.float64)\n",
    "\n",
    "            self.samples=tf.stack(values=samples,axis=1) # (self.N, num_samples) to (num_samples, self.N): Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples, self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            samples:         tf.Tensor\n",
    "                             a tf.placeholder of shape (number of samples,system-size)\n",
    "                             containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                             dimension of the input space\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                             the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_state=self.rnn.zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn(inputs, rnn_state)\n",
    "                    output=self.dense(rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class RNNProbabilityDilated(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[2],scope='RNNwavefunction', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int, size of the lattice\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                         number of units per RNN layer\n",
    "            scope:       str\n",
    "                         the name of the name-space scope\n",
    "            Seed         int, pseudo random generate to guarantee reproducibility\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph=tf.Graph()\n",
    "        self.scope=scope #Label of the RNN wavefunction\n",
    "        self.N=systemsize #Number of nodes\n",
    "        self.numlayers = len(units)\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        #Defining the neural network\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn=[[cell(num_units = units[i], activation = activation,name=\"rnn_\"+str(n)+str(i),dtype=tf.float64) for n in range(self.N)] for i in range(self.numlayers)]\n",
    "                self.dense = [tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='wf_dense'+str(n)) for n in range(self.N)] #Define the Fully-Connected layer followed by a Softmax\n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "            numsamples:      int\n",
    "                             number of samples to be produced\n",
    "            inputdim:        int\n",
    "                             hilbert space dimension\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                             the samples in integer encoding\n",
    "        \"\"\"\n",
    "        with self.graph.as_default(): # Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            probs = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "                #Initial input to feed to the rnn\n",
    "\n",
    "                self.inputdim=inputs.shape[1]\n",
    "                self.outputdim=self.inputdim\n",
    "                self.numsamples=inputs.shape[0]\n",
    "\n",
    "                rnn_states = []\n",
    "\n",
    "                for i in range(self.numlayers):\n",
    "                    for n in range(self.N):\n",
    "                        rnn_states.append(self.rnn[i][n].zero_state(self.numsamples,dtype=tf.float64)) #Initialize the RNN hidden state\n",
    "\n",
    "                #zero state returns a zero filled tensor withs shape = (self.numsamples, num_units)\n",
    "\n",
    "                for n in range(self.N):\n",
    "\n",
    "                    rnn_output = inputs\n",
    "\n",
    "                    for i in range(self.numlayers):\n",
    "                        if (n-2**i)>=0:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i+((n-2**i)*self.numlayers)]) #Compute the next hidden states\n",
    "                        else:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i])\n",
    "\n",
    "                    output=self.dense[n](rnn_output) \n",
    "                    probs.append(output)\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,]) #Sample from the probability\n",
    "                    samples.append(sample_temp)\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim,dtype = tf.float64)\n",
    "\n",
    "        probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "        self.samples=tf.stack(values=samples,axis=1) # (self.N, num_samples) to (num_samples, self.N): Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "        one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "        self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples,self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "            samples:         tf.Tensor\n",
    "                             a tf.placeholder of shape (number of samples,systemsize)\n",
    "                             containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                             dimension of the input space\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                             the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_states = []\n",
    "\n",
    "                for i in range(self.numlayers):\n",
    "                    for n in range(self.N):\n",
    "                        rnn_states.append(self.rnn[i][n].zero_state(self.numsamples,dtype=tf.float64)) #Initialize the RNN hidden state\n",
    "\n",
    "                #zero state returns a zero filled tensor withs shape = (self.numsamples, num_units)\n",
    "\n",
    "                for n in range(self.N):\n",
    "\n",
    "                    rnn_output = inputs\n",
    "\n",
    "                    for i in range(self.numlayers):\n",
    "                        if (n-2**i)>=0:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i+((n-2**i)*self.numlayers)]) #Compute the next hidden states\n",
    "                        else:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i])\n",
    "\n",
    "                    output=self.dense[n](rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.cast(tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1]),tf.float64)\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class vca:\n",
    "\n",
    "    # allows user to decide what type of RNN unit to use among basicRNN, lstmRNN, gru\n",
    "    rnn_cells = {\n",
    "        'basic': tf.compat.v1.nn.rnn_cell.BasicRNNCell, \n",
    "        'lstm': tf.compat.v1.nn.rnn_cell.LSTMCell,\n",
    "        'gru': tf.compat.v1.nn.rnn_cell.GRUCell,\n",
    "        }\n",
    "\n",
    "    def __init__(self, N, n_layers, n_warmup, n_anneal, n_train, qubo, RNNtype, rnn_unit, T0):\n",
    "        \"\"\"\n",
    "        N - system size of optimization problem\n",
    "        n_layers - number of RNN layers \n",
    "        n_warmup - warmup iterations\n",
    "        n_anneal - temperature annealing iterations\n",
    "        n_train - RNN model training iterations\n",
    "        qubo - qubo matrix of an max-cut graph used to compute the energy of solutions to the max-cut problem\n",
    "        RNNtype - general architecture of RNN among \n",
    "        {'ws': single-chain/VCA-Vanilla with shared parameters at every RNN cell, \n",
    "        'nws': single-chain/VCA-Vanilla with dedicated parameters at every RNN cell, \n",
    "        'dilated': VCA-Dilated with dedicated parameters}\n",
    "        rnn_unit - type of RNN cell among {\"basicRNN\", \"GRU\", \"LSTM\"}\n",
    "        T0 - initial temperature\n",
    "        \"\"\"\n",
    "        #Seeding for reproducibility purposes\n",
    "        seed = 111\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "        tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "\n",
    "        self.N = N\n",
    "        self.num_units = 40     # size of RNN hidden units\n",
    "        self.numlayers = n_layers\n",
    "        self.numsamples = 50    # number of samples to generate per training step during backprop\n",
    "\n",
    "        #Defining the other parameters\n",
    "        self.units = [self.num_units]*self.numlayers # defines the number of hidden units at each layer (for Dilated)\n",
    "        self.lr = np.float64(1e-4)\n",
    "        self.activation_function = tf.nn.elu\n",
    "        self.rnn_cell = vca.rnn_cells[rnn_unit]\n",
    "\n",
    "        self.num_warmup = n_warmup\n",
    "        self.num_anneal = n_anneal\n",
    "        self.num_train = n_train\n",
    "\n",
    "        self.T0 = T0\n",
    "        self.qubo = qubo\n",
    "\n",
    "        print('\\n')\n",
    "        print(\"Number of spins = {}\".format(self.N))\n",
    "        print('Number of samples = {}'.format(self.numsamples))\n",
    "        print(\"Initial temperature {}\".format(self.T0))\n",
    "\n",
    "        print('\\nWamup steps = {}'.format(self.num_warmup))\n",
    "        print('Annealing steps = {}'.format(self.num_anneal))\n",
    "        print('Training steps at a fixed temperature= {}'.format(self.num_train))\n",
    "        print('Total training steps = {}\\n'.format(self.num_warmup+self.num_anneal*self.num_train))\n",
    "\n",
    "        print('Seed = ', seed)\n",
    "        print(\"Number of layers = {0}\\n\".format(self.numlayers))\n",
    "\n",
    "        # Intitializing the RNN-----------\n",
    "        # create either the weight-sharing RNN object or non-weight-sharing RNN object \n",
    "        if RNNtype == 'ws':\n",
    "            self.PRNN = RNNProbabilityWS(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed) #contains the graph with the RNNs\n",
    "        elif RNNtype == 'nws':\n",
    "            self.PRNN = RNNProbabilityNWS(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed)\n",
    "        elif RNNtype == 'dilated':\n",
    "            self.PRNN = RNNProbabilityDilated(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed)\n",
    "\n",
    "#Loading previous trainings----------\n",
    "    ### To be implemented\n",
    "#------------------------------------\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        vca.run() builds an RNN architecture based on the choice of hyperparameters provided in vca.__init__(). The model then autoregressively generates numsamples (see __init__()) \n",
    "        solutions/samples to the optimization problem from which the energies, along with the probabilities of samples and temperature, is used to compute the cost function. \n",
    "        Using this cost, the RNN parameters are updated. This process repeats over n_warmup steps with the temperature fixed at T0, and subsequently over n_anneal*n_train steps \n",
    "        with the temperature decreasing at every n_anneal step.\n",
    "        energies_final - energies of 500,000 samples (see line 627) generated after training the model \n",
    "        samples_final - samples (or solutions) in the form binary vectors generated after training the model\n",
    "        \"\"\"\n",
    "        #Building the graph -------------------\n",
    "        with tf.compat.v1.variable_scope(self.PRNN.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            with self.PRNN.graph.as_default():\n",
    "\n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "                learningrate_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=[])\n",
    "                learningrate = tf.compat.v1.train.exponential_decay(learningrate_placeholder, global_step, 100, 1.0, staircase=True)\n",
    "                optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learningrate)\n",
    "\n",
    "                E_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=[self.numsamples])\n",
    "                samples_placeholder = tf.compat.v1.placeholder(dtype=tf.int32,shape=[self.numsamples,self.N])\n",
    "                log_probs_tensor = self.PRNN.log_probability(samples_placeholder,inputdim=2)\n",
    "                T_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=())\n",
    "\n",
    "                #Fake Cost function = energy_term + entropy_term\n",
    "                F = E_placeholder+T_placeholder*log_probs_tensor\n",
    "                fake_cost = tf.reduce_mean(log_probs_tensor*tf.stop_gradient(F)) - tf.reduce_mean(log_probs_tensor)*tf.reduce_mean(tf.stop_gradient(F)) \n",
    "                #fake_cost != F_RNN\n",
    "                \n",
    "                gradients, variables = zip(*optimizer.compute_gradients(fake_cost))\n",
    "                #Calculate Gradients---------------\n",
    "\n",
    "                optstep = optimizer.apply_gradients(zip(gradients,variables), global_step = global_step)\n",
    "\n",
    "                init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "        #Starting Session------------\n",
    "        #GPU management\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess=tf.compat.v1.Session(graph=self.PRNN.graph, config=config)\n",
    "        sess.run(init)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(self.PRNN.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            with self.PRNN.graph.as_default():\n",
    "\n",
    "                # containers to hold warm up energies\n",
    "                WU_meanEnergy = []\n",
    "                WU_varEnergy = []\n",
    "                \n",
    "\n",
    "                samplesandprobs = self.PRNN.sample(numsamples=self.numsamples,inputdim=2)\n",
    "                samples = np.ones((self.numsamples, self.N), dtype=np.int32)\n",
    "\n",
    "                # counter for time\n",
    "                time_count = 0\n",
    "                T = self.T0\n",
    "\n",
    "                # Warmup Loop\n",
    "                for it in range(self.num_warmup):\n",
    "                    \n",
    "                    # start the timer\n",
    "                    if it == 0:\n",
    "                        start = time.time()\n",
    "                    \n",
    "                    samples, log_probs = sess.run(samplesandprobs)\n",
    "                    energies = hamiltonian(samples, self.qubo)\n",
    "\n",
    "                    meanE = np.mean(energies)\n",
    "                    varE = np.var(energies)\n",
    "\n",
    "                    # append the elements\n",
    "                    WU_meanEnergy.append(meanE)\n",
    "                    WU_varEnergy.append(varE)\n",
    "\n",
    "                    # compute free energy and its variance\n",
    "                    meanF = np.mean(energies + T*log_probs)\n",
    "                    varF = np.var(energies + T*log_probs)\n",
    "\n",
    "                    # Do gradient step\n",
    "                    sess.run(optstep,feed_dict={E_placeholder:energies,samples_placeholder:samples,learningrate_placeholder: self.lr, T_placeholder:T})\n",
    "\n",
    "                    if it%5==0:\n",
    "                        print('WARM UP PHASE')\n",
    "                        print('mean(E): {0}, mean(F): {1}, var(E): {2}, var(F): {3}, #samples {4}, #Training step {5}'.format(meanE,meanF,varE,varF,self.numsamples, it))\n",
    "                        print(\"Temperature: \", T)        \n",
    "\n",
    "                    if time_count%5 == 0 and time_count>0:\n",
    "                        print(\"Elapsed time is =\", time.time()-start, \" seconds\")\n",
    "                        print('\\n\\n')\n",
    "\n",
    "                    time_count += 1    \n",
    "            \n",
    "                # containers for annealing loop\n",
    "                meanEnergy=[]\n",
    "                varEnergy=[]\n",
    "                varFreeEnergy = []\n",
    "                meanFreeEnergy = []\n",
    "                temperatures = []\n",
    "                \n",
    "                # annealing loop\n",
    "                for it0 in range(self.num_anneal):\n",
    "\n",
    "                    # reduce the temperature\n",
    "                    T = self.T0*(1-it0/self.num_anneal)\n",
    "                    temperatures.append(T)\n",
    "                    \n",
    "                    # training loop\n",
    "                    for it1 in range(self.num_train):\n",
    "\n",
    "                        samples, log_probs = sess.run(samplesandprobs)\n",
    "\n",
    "                        energies = hamiltonian(samples, self.qubo)\n",
    "\n",
    "                        meanE = np.mean(energies)\n",
    "                        varE = np.var(energies)\n",
    "\n",
    "                        #adding elements to be saved\n",
    "                        meanEnergy.append(meanE)\n",
    "                        varEnergy.append(varE)\n",
    "\n",
    "                        meanF = np.mean(energies + T*log_probs)\n",
    "                        varF = np.var(energies + T*log_probs)\n",
    "\n",
    "                        meanFreeEnergy.append(meanF)\n",
    "                        varFreeEnergy.append(varF)\n",
    "\n",
    "                        if it1%5==0:\n",
    "                            print('ANNEALING PHASE')\n",
    "                            print('mean(E): {0}, mean(F): {1}, var(E): {2}, var(F): {3}, #samples {4}, #Training step {5}'.format(meanE,meanF,varE,varF,self.numsamples, it))\n",
    "                            print(\"Temperature: \", T)\n",
    "\n",
    "                        # Do gradient step\n",
    "                        sess.run(optstep,feed_dict={E_placeholder:energies,samples_placeholder:samples,learningrate_placeholder: self.lr, T_placeholder:T})\n",
    "\n",
    "                        if time_count%5 == 0 and time_count>0:\n",
    "                            print(\"Ellapsed time is =\", time.time()-start, \" seconds\")\n",
    "                            print('\\n\\n')\n",
    "\n",
    "                        time_count += 1\n",
    "\n",
    "                # when training is done, generate 500000 samples, 50000 at a time\n",
    "                samples_per_step = 50000\n",
    "                n_steps = 10\n",
    "                samplesandprobs_final = self.PRNN.sample(numsamples=samples_per_step, inputdim=2)\n",
    "\n",
    "                samples_final = np.ones((samples_per_step*n_steps, self.N), dtype=np.int32)\n",
    "                energies_final = np.ones((samples_per_step*n_steps))\n",
    "\n",
    "                for i in range(n_steps):\n",
    "                    samples_step, _ = sess.run(samplesandprobs_final)\n",
    "                    energies_step = hamiltonian(samples_step, self.qubo)\n",
    "                    samples_final[(i)*samples_per_step : (i+1)*samples_per_step] = samples_step\n",
    "                    energies_final[(i)*samples_per_step : (i+1)*samples_per_step] = energies_step\n",
    "                \n",
    "                print(\"500,000 samples generated after training\")\n",
    "                    \n",
    "        return energies_final, samples_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filepath of the Max-Cut graph instance below\n",
    "filepath = \"\"\n",
    "assert(os.path.exists(filepath)) # check if file exists in path\n",
    "graph, N = read_graph(filepath)\n",
    "qubo = formulate_qubo(graph)\n",
    "# VCA parameters\n",
    "\"\"\"\n",
    "n_warmup - number of training steps at the initial temperature T=T0\n",
    "n_anneal - duration of the annealing procedure\n",
    "n_train - number of training step during backprop after every annealing step\n",
    "dilated_layers - number of dilated layers for VCA-Dilated. Note: for other architectures, layers = 1\n",
    "\"\"\"\n",
    "n_warmup = 2000\n",
    "n_anneal = 128\n",
    "n_train = 5\n",
    "dilated_layers = np.int32(np.ceil(np.log2(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNtype specifies the RNN architecture among {'ws', 'nws', 'dilated'} where\n",
    "# 'ws' - weight-sharing RNN parameters\n",
    "# 'nws' - independent RNN parameters at every RNN cell\n",
    "# 'dilated' - independent RNN parameters at every RNN cell with a dilatedRNN structure\n",
    "# rnn_unit specifies the RNN cell type among {'basic', 'lstm', 'gru'} specifies the RNN cell type among {'basic', 'lstm', 'gru'}\n",
    "\n",
    "# VCA-Dilated\n",
    "model = vca(N=N, n_layers=dilated_layers, n_warmup=n_warmup, n_anneal=n_anneal, n_train=n_train, qubo=qubo, RNNtype='dilated', rnn_unit='basic', T0=2)\n",
    "energies_dilated, samples_dilated = model.run()\n",
    "\n",
    "# VCA-Vanilla\n",
    "model = vca(N=N, n_layers=1, n_warmup=n_warmup, n_anneal=n_anneal, n_train=n_train, qubo=qubo, RNNtype='nws', rnn_unit='basic', T0=2)\n",
    "energies_vanilla, samples_vanilla = model.run()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c17fe89368f3df3b6fd3fdfd57958e43f86c5fba66a270cb992506caf886a193"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ds': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
