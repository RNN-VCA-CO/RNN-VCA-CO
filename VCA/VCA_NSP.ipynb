{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) #stop displaying tensorflow warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSP Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note:\n",
    "This QUBO matrix derivation of NSP has been done by Dwave.\n",
    "\"\"\"\n",
    "def dwave_nsp_qubo(n_nurses, n_days):\n",
    "    \"\"\"\n",
    "    dwave_nsp_qubo takes in the number of nurses and days as parameters and returns the derive qubo matrix.\n",
    "    n_nurses - number of nurses in NSP configuration\n",
    "    n_days - number of days in NSP configuration\n",
    "    \"\"\"\n",
    "    # row, col size of J matrix\n",
    "    size = n_nurses * n_days\n",
    "    # dictionary for mapping (nurse, day) to the appropriate index in a vector representation of solution\n",
    "    d = {(nurse,day):nurse*n_days+day for nurse in range(n_nurses) for day in range(n_days)}\n",
    "\n",
    "    # HNC correlation coeff\n",
    "    a = 3.5\n",
    "    # HSC coeff and constants\n",
    "    hsc_coeff = 1.3\n",
    "    workload = 1\n",
    "    effort = 1\n",
    "    # SNC coeff and constants\n",
    "    snc_coeff = .3\n",
    "    min_duty_days = int(n_days/n_nurses)\n",
    "    preference = 1\n",
    "\n",
    "    # qubo matrix\n",
    "    J = np.zeros(shape=(size,size))\n",
    "\n",
    "    # placing the HNC entries in J \n",
    "    for nurse in range(n_nurses):\n",
    "        for day in range(n_days-1):\n",
    "            i, j = d[(nurse,day)], d[(nurse,day+1)]\n",
    "            J[i][j] = a\n",
    "\n",
    "    # placing the HSC entries in J \n",
    "    for nurse in range(n_nurses):\n",
    "        for day in range(n_days):\n",
    "            index = d[(nurse, day)]\n",
    "            J[index, index] += hsc_coeff * (effort**2 - (2*workload*effort))\n",
    "\n",
    "    for day in range(n_days):\n",
    "        for nurse1 in range(n_nurses):\n",
    "            for nurse2 in range(nurse1 + 1, n_nurses):\n",
    "                index1 = d[(nurse1, day)]\n",
    "                index2 = d[(nurse2, day)]\n",
    "                J[index1, index2] += 2*hsc_coeff*effort**2\n",
    "\n",
    "    # placing the SNC entries in J \n",
    "    for nurse in range(n_nurses):\n",
    "        for day in range(n_days):\n",
    "            index = d[(nurse, day)]\n",
    "            J[index, index] += snc_coeff * (preference**2 - (2*preference*min_duty_days))\n",
    "\n",
    "    for nurse in range(n_nurses):\n",
    "        for day1 in range(n_days):\n",
    "            for day2 in range(day1 + 1, n_days):\n",
    "                index1 = d[(nurse, day1)]\n",
    "                index2 = d[(nurse, day2)]\n",
    "                J[index1, index2] += 2*snc_coeff*preference**2\n",
    "\n",
    "    offset = hsc_coeff*n_days*workload**2 + snc_coeff*n_nurses*min_duty_days**2\n",
    "    \n",
    "    return J, offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCA Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for experimental reproducibility\n",
    "seed = 111\n",
    "random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "np.random.seed(seed)  # numpy pseudo-random generator\n",
    "tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "\n",
    "def nsp_matrixelements(samples, qubo, offset):\n",
    "    \"\"\"\n",
    "    nsp_matrixelements returns the energies of an array of solutions.\n",
    "    samples - array of solutions\n",
    "    qubo - QUBO matrix of NSP configuration\n",
    "    offset - offset (derived from the original NSP hamiltonian formulation) added to the result of a scalar product\n",
    "    RETURNS energies - returns the energy of the samples\n",
    "    \"\"\"\n",
    "    xJ = np.dot(samples, qubo)\n",
    "    energies = np.einsum('ij,ij->i', xJ, samples)   # numpy einsum computation\n",
    "    energies += offset\n",
    "\n",
    "    return energies\n",
    "\n",
    "\"\"\"\n",
    "RNNProbability classes\n",
    "\"\"\"\n",
    "class RNNProbabilityNWS(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[10],scope='RNNProbability', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int\n",
    "                        number of sites\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                        number of units per RNN layer\n",
    "            scope:       str\n",
    "                        the name of the name-space scope\n",
    "            activation:  activation of the RNN cell\n",
    "            seed:        pseudo-random number generator\n",
    "        \"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        self.scope = scope #Label of the RNN probability\n",
    "        self.N = systemsize #Number of sites of the 1D chain\n",
    "\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        # Defining the neural network\n",
    "\n",
    "        # different RNN block being used at every site\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn=[tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell(units[i],activation = activation,name='RNN_{0}{1}'.format(i,n), dtype = tf.float64) for i in range(len(units))]) for n in range(self.N)]\n",
    "                self.dense = [tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='PRNN_dense_{0}'.format(n), dtype = tf.float64) for n in range(self.N)]\n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            numsamples:      int\n",
    "                            number of samples to be produced\n",
    "            inputdim:        int\n",
    "                            hilbert space dimension\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:         a tuple (samples,log-probs)\n",
    "\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                            the samples in integer encoding\n",
    "            log-probs        tf.Tensor of shape (numsamples,)\n",
    "                            the log-probability of each sample\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default(): # Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                \n",
    "                # b = zeros ndarray with shape [numsamples,inputdim] \n",
    "                # b = state of one spin for all the samples, this command above makes all the samples having 1 in the first component and 0 in the second.\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "\n",
    "                probs=[]\n",
    "\n",
    "                # inputs = b ndarray used as a template to create equivalent tensor\n",
    "                # Initial input to feed to the rnn\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "\n",
    "                self.inputdim=inputs.shape[1]   # 2\n",
    "                self.outputdim=self.inputdim    # 2\n",
    "                self.numsamples=inputs.shape[0] # M \n",
    "\n",
    "                rnn_state=self.rnn[0].zero_state(self.numsamples,dtype=tf.float64)  # [numsamples,num_units]\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn[n](inputs, rnn_state)  # elu activation function output of shape [numsamples,num_units] \n",
    "                    output=self.dense[n](rnn_output)    # softmax probability of +1 and -1 spins [numsamples, 2]\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,])   # sampled nth position spin; values are either 0 or 1 [numsamples,]\n",
    "                    probs.append(output) \n",
    "                    samples.append(sample_temp)     # [N, numsamples]\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim, dtype = tf.float64)     # one-hot encoded vector input for subsequent RNN [numsamples, 2]\n",
    "\n",
    "            self.samples=tf.stack(values=samples,axis=1) # [self.N, num_samples] to [num_samples, self.N]: Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples, self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            samples:         tf.Tensor\n",
    "                            a tf.placeholder of shape (number of samples,system-size)\n",
    "                            containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                            dimension of the input space\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                            the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_state=self.rnn[0].zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn[n](inputs, rnn_state)\n",
    "                    output=self.dense[n](rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class RNNProbabilityWS(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[10],scope='RNNProbability', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int\n",
    "                         number of sites\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                         number of units per RNN layer\n",
    "            scope:       str\n",
    "                         the name of the name-space scope\n",
    "            activation:  activation function of the RNN cell\n",
    "            seed:        pseudo-random number generator\n",
    "        \"\"\"\n",
    "        self.graph=tf.Graph()\n",
    "        self.scope=scope #Label of the RNN probability\n",
    "        self.N=systemsize #Number of sites of the 1D chain\n",
    "\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        #Defining the neural network\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell(units[i],activation = activation,name='RNN_{0}'.format(i), dtype = tf.float64) for i in range(len(units))])\n",
    "                self.dense = tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='PRNN_dense', dtype = tf.float64) \n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            numsamples:      int\n",
    "                             number of samples to be produced\n",
    "            inputdim:        int\n",
    "                             hilbert space dimension\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:         a tuple (samples,log-probs)\n",
    "\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                             the samples in integer encoding\n",
    "            log-probs        tf.Tensor of shape (numsamples,)\n",
    "                             the log-probability of each sample\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default(): #Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "                #b = state of one spin for all the samples, this command above makes all the samples having 1 in the first component and 0 in the second.\n",
    "                probs=[]\n",
    "\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "                #Initial input to feed to the rnn\n",
    "\n",
    "                self.inputdim=inputs.shape[1]\n",
    "                self.outputdim=self.inputdim\n",
    "                self.numsamples=inputs.shape[0]\n",
    "\n",
    "                rnn_state=self.rnn.zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn(inputs, rnn_state)\n",
    "                    output=self.dense(rnn_output)\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,])\n",
    "                    probs.append(output)\n",
    "                    samples.append(sample_temp)\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim, dtype = tf.float64)\n",
    "\n",
    "            self.samples=tf.stack(values=samples,axis=1) # (self.N, num_samples) to (num_samples, self.N): Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples, self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "\n",
    "            samples:         tf.Tensor\n",
    "                             a tf.placeholder of shape (number of samples,system-size)\n",
    "                             containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                             dimension of the input space\n",
    "\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                             the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_state=self.rnn.zero_state(self.numsamples,dtype=tf.float64)\n",
    "\n",
    "                for n in range(self.N):\n",
    "                    rnn_output, rnn_state = self.rnn(inputs, rnn_state)\n",
    "                    output=self.dense(rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class RNNProbabilityDilated(object):\n",
    "    def __init__(self,systemsize,cell=None,activation=tf.nn.relu,units=[2],scope='RNNwavefunction', seed = 111):\n",
    "        \"\"\"\n",
    "            systemsize:  int, size of the lattice\n",
    "            cell:        a tensorflow RNN cell\n",
    "            units:       list of int\n",
    "                         number of units per RNN layer\n",
    "            scope:       str\n",
    "                         the name of the name-space scope\n",
    "            Seed         int, pseudo random generate to guarantee reproducibility\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph=tf.Graph()\n",
    "        self.scope=scope #Label of the RNN wavefunction\n",
    "        self.N=systemsize #Number of nodes\n",
    "        self.numlayers = len(units)\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "\n",
    "        #Defining the neural network\n",
    "        with self.graph.as_default():\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "                self.rnn=[[cell(num_units = units[i], activation = activation,name=\"rnn_\"+str(n)+str(i),dtype=tf.float64) for n in range(self.N)] for i in range(self.numlayers)]\n",
    "                self.dense = [tf.compat.v1.layers.Dense(2,activation=tf.nn.softmax,name='wf_dense'+str(n)) for n in range(self.N)] #Define the Fully-Connected layer followed by a Softmax\n",
    "\n",
    "    def sample(self,numsamples,inputdim):\n",
    "        \"\"\"\n",
    "            generate samples from a probability distribution parametrized by a recurrent network\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "            numsamples:      int\n",
    "                             number of samples to be produced\n",
    "            inputdim:        int\n",
    "                             hilbert space dimension\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            samples:         tf.Tensor of shape (numsamples,systemsize)\n",
    "                             the samples in integer encoding\n",
    "        \"\"\"\n",
    "        with self.graph.as_default(): # Call the default graph, used if willing to create multiple graphs.\n",
    "            samples = []\n",
    "            probs = []\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                b=np.zeros((numsamples,inputdim)).astype(np.float64)\n",
    "\n",
    "                inputs=tf.constant(dtype=tf.float64,value=b,shape=[numsamples,inputdim]) #Feed the table b in tf.\n",
    "                #Initial input to feed to the rnn\n",
    "\n",
    "                self.inputdim=inputs.shape[1]\n",
    "                self.outputdim=self.inputdim\n",
    "                self.numsamples=inputs.shape[0]\n",
    "\n",
    "                rnn_states = []\n",
    "\n",
    "                for i in range(self.numlayers):\n",
    "                    for n in range(self.N):\n",
    "                        rnn_states.append(self.rnn[i][n].zero_state(self.numsamples,dtype=tf.float64)) #Initialize the RNN hidden state\n",
    "\n",
    "                #zero state returns a zero filled tensor withs shape = (self.numsamples, num_units)\n",
    "\n",
    "                for n in range(self.N):\n",
    "\n",
    "                    rnn_output = inputs\n",
    "\n",
    "                    for i in range(self.numlayers):\n",
    "                        if (n-2**i)>=0:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i+((n-2**i)*self.numlayers)]) #Compute the next hidden states\n",
    "                        else:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i])\n",
    "\n",
    "                    output=self.dense[n](rnn_output) \n",
    "                    probs.append(output)\n",
    "                    sample_temp=tf.reshape(tf.compat.v1.multinomial(tf.math.log(output),num_samples=1),[-1,]) #Sample from the probability\n",
    "                    samples.append(sample_temp)\n",
    "                    inputs=tf.one_hot(sample_temp,depth=self.outputdim,dtype = tf.float64)\n",
    "\n",
    "        probs=tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1])\n",
    "        self.samples=tf.stack(values=samples,axis=1) # (self.N, num_samples) to (num_samples, self.N): Generate self.numsamples vectors of size self.N spin containing 0 or 1\n",
    "        one_hot_samples=tf.one_hot(self.samples,depth=self.inputdim, dtype = tf.float64)\n",
    "        self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "        return self.samples,self.log_probs\n",
    "\n",
    "    def log_probability(self,samples,inputdim):\n",
    "        \"\"\"\n",
    "            calculate the log-probabilities of ```samples``\n",
    "            ------------------------------------------------------------------------\n",
    "            Parameters:\n",
    "            samples:         tf.Tensor\n",
    "                             a tf.placeholder of shape (number of samples,systemsize)\n",
    "                             containing the input samples in integer encoding\n",
    "            inputdim:        int\n",
    "                             dimension of the input space\n",
    "            ------------------------------------------------------------------------\n",
    "            Returns:\n",
    "            log-probs        tf.Tensor of shape (number of samples,)\n",
    "                             the log-probability of each sample\n",
    "            \"\"\"\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputdim=inputdim\n",
    "            self.outputdim=self.inputdim\n",
    "\n",
    "            self.numsamples=tf.shape(samples)[0]\n",
    "\n",
    "            inputs=tf.zeros((self.numsamples, self.inputdim), dtype=tf.float64)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(self.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "                probs=[]\n",
    "\n",
    "                rnn_states = []\n",
    "\n",
    "                for i in range(self.numlayers):\n",
    "                    for n in range(self.N):\n",
    "                        rnn_states.append(self.rnn[i][n].zero_state(self.numsamples,dtype=tf.float64)) #Initialize the RNN hidden state\n",
    "\n",
    "                #zero state returns a zero filled tensor withs shape = (self.numsamples, num_units)\n",
    "\n",
    "                for n in range(self.N):\n",
    "\n",
    "                    rnn_output = inputs\n",
    "\n",
    "                    for i in range(self.numlayers):\n",
    "                        if (n-2**i)>=0:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i+((n-2**i)*self.numlayers)]) #Compute the next hidden states\n",
    "                        else:\n",
    "                            rnn_output, rnn_states[i + n*self.numlayers] = self.rnn[i][n](rnn_output, rnn_states[i])\n",
    "\n",
    "                    output=self.dense[n](rnn_output)\n",
    "                    probs.append(output)\n",
    "                    inputs=tf.reshape(tf.one_hot(tf.reshape(tf.slice(samples,begin=[np.int32(0),np.int32(n)],size=[np.int32(-1),np.int32(1)]),shape=[self.numsamples]),depth=self.outputdim,dtype = tf.float64),shape=[self.numsamples,self.inputdim])\n",
    "\n",
    "            probs=tf.cast(tf.transpose(tf.stack(values=probs,axis=2),perm=[0,2,1]),tf.float64)\n",
    "            one_hot_samples=tf.one_hot(samples,depth=self.inputdim, dtype = tf.float64)\n",
    "\n",
    "            self.log_probs=tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs,one_hot_samples),axis=2)),axis=1)\n",
    "\n",
    "            return self.log_probs\n",
    "\n",
    "class vca:\n",
    "\n",
    "    # allows user to decide what type of RNN unit to use among basicRNN, lstmRNN, gru\n",
    "    rnn_cells = {\n",
    "        'basic': tf.compat.v1.nn.rnn_cell.BasicRNNCell, \n",
    "        'lstm': tf.compat.v1.nn.rnn_cell.LSTMCell,\n",
    "        'gru': tf.compat.v1.nn.rnn_cell.GRUCell,\n",
    "        }\n",
    "\n",
    "\n",
    "    def __init__(self, N, n_layers, n_warmup, n_anneal, n_train, J, offset, RNNtype, rnn_unit, T0):\n",
    "\n",
    "        #Seeding for reproducibility purposes\n",
    "        seed = 111\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        random.seed(seed)  # `python` built-in pseudo-random generator\n",
    "        np.random.seed(seed)  # numpy pseudo-random generator\n",
    "        tf.compat.v1.set_random_seed(seed)  # tensorflow pseudo-random generator\n",
    "\n",
    "        \"\"\"\n",
    "        N - system size of optimization problem\n",
    "        n_layers - number of RNN layers \n",
    "        n_warmup - warmup iterations\n",
    "        n_anneal - temperature annealing iterations\n",
    "        n_train - RNN model training iterations\n",
    "        J - qubo matrix of optimization problem\n",
    "        offset - real constant rderived from the NSP energy function. This is used to compute the energy of a solution.\n",
    "        RNNtype - general architecture of RNN among {'ws': VCA-Vanilla with shared parameters, \n",
    "        'nws': VCA-Vanilla with dedicated parameters, 'dilated':VCA-Dilated with dedicated parameters}; declaared by string\n",
    "        rnn_unit - type of RNN cell; declared by  string\n",
    "        T0 - initial temperature\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = N\n",
    "        self.num_units = 40     # size of RNN hidden units\n",
    "        self.numlayers = n_layers\n",
    "        self.numsamples = 50    # number of samples to generate per training step during backprop\n",
    "\n",
    "        #Defining the other parameters\n",
    "        self.units = [self.num_units]*self.numlayers # defines the number of hidden units at each layer (for Dilated)\n",
    "        self.lr = np.float64(1e-4)\n",
    "        self.activation_function = tf.nn.elu\n",
    "        self.rnn_cell = vca.rnn_cells[rnn_unit]\n",
    "\n",
    "        self.num_warmup = n_warmup\n",
    "        self.num_anneal = n_anneal\n",
    "        self.num_train = n_train\n",
    "\n",
    "        self.T0 = T0\n",
    "        self.J = J\n",
    "        self.offset = offset\n",
    "\n",
    "        print('\\n')\n",
    "        print(\"Number of spins = {}\".format(self.N))\n",
    "        print('Number of samples = {}'.format(self.numsamples))\n",
    "        print(\"Initial temperature {}\".format(self.T0))\n",
    "\n",
    "        print('\\nWamup steps = {}'.format(self.num_warmup))\n",
    "        print('Annealing steps = {}'.format(self.num_anneal))\n",
    "        print('Training steps at a fixed temperature= {}'.format(self.num_train))\n",
    "        print('Total training steps = {}\\n'.format(self.num_warmup+self.num_anneal*self.num_train))\n",
    "\n",
    "        print('Seed = ', seed)\n",
    "        print(\"Number of layers = {0}\\n\".format(self.numlayers))\n",
    "\n",
    "        # Intitializing the RNN-----------\n",
    "        # create either the weight-sharing RNN object or non-weight-sharing RNN object \n",
    "        if RNNtype == 'ws':\n",
    "            self.PRNN = RNNProbabilityWS(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed) #contains the graph with the RNNs\n",
    "        elif RNNtype == 'nws':\n",
    "            self.PRNN = RNNProbabilityNWS(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed)\n",
    "        elif RNNtype == 'dilated':\n",
    "            self.PRNN = RNNProbabilityDilated(self.N, units=self.units, cell=self.rnn_cell, activation=self.activation_function, seed=seed)\n",
    "\n",
    "#Loading previous trainings----------\n",
    "    ### To be implemented\n",
    "#------------------------------------\n",
    "    def run(self):\n",
    "\n",
    "        #--------------------------\n",
    "        #List to contain the energies and the variances\n",
    "        # meanEnergy=[]\n",
    "        # varEnergy=[]\n",
    "\n",
    "        #Building the graph -------------------\n",
    "        with tf.compat.v1.variable_scope(self.PRNN.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            with self.PRNN.graph.as_default():\n",
    "\n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "                learningrate_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=[])\n",
    "                learningrate = tf.compat.v1.train.exponential_decay(learningrate_placeholder, global_step, 100, 1.0, staircase=True)\n",
    "                optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learningrate)\n",
    "\n",
    "                E_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=[self.numsamples])\n",
    "                samples_placeholder = tf.compat.v1.placeholder(dtype=tf.int32,shape=[self.numsamples,self.N])\n",
    "                log_probs_tensor = self.PRNN.log_probability(samples_placeholder,inputdim=2)\n",
    "                T_placeholder = tf.compat.v1.placeholder(dtype=tf.float64,shape=())\n",
    "\n",
    "                #Fake Cost function = energy_term + entropy_term\n",
    "                F = E_placeholder+T_placeholder*log_probs_tensor\n",
    "                fake_cost = tf.reduce_mean(log_probs_tensor*tf.stop_gradient(F)) - tf.reduce_mean(log_probs_tensor)*tf.reduce_mean(tf.stop_gradient(F)) \n",
    "                #fake_cost != F_RNN\n",
    "                \n",
    "                gradients, variables = zip(*optimizer.compute_gradients(fake_cost))\n",
    "                #Calculate Gradients---------------\n",
    "\n",
    "                optstep = optimizer.apply_gradients(zip(gradients,variables), global_step = global_step)\n",
    "\n",
    "                init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "        #Starting Session------------\n",
    "        #GPU management\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess=tf.compat.v1.Session(graph=self.PRNN.graph, config=config)\n",
    "        sess.run(init)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(self.PRNN.scope,reuse=tf.compat.v1.AUTO_REUSE):\n",
    "            with self.PRNN.graph.as_default():\n",
    "\n",
    "                # containers to hold warm up energies\n",
    "                WU_meanEnergy = []\n",
    "                WU_varEnergy = []\n",
    "                \n",
    "\n",
    "                samplesandprobs = self.PRNN.sample(numsamples=self.numsamples,inputdim=2)\n",
    "                samples = np.ones((self.numsamples, self.N), dtype=np.int32)\n",
    "\n",
    "                # counter for time\n",
    "                time_count = 0\n",
    "                T = self.T0\n",
    "\n",
    "                # Warmup Loop\n",
    "                for it in range(self.num_warmup):\n",
    "                    \n",
    "                    # start the timer\n",
    "                    if it == 0:\n",
    "                        start = time.time()\n",
    "                    \n",
    "                    samples, log_probs = sess.run(samplesandprobs)\n",
    "                    energies = nsp_matrixelements(samples, self.J, offset)\n",
    "\n",
    "                    meanE = np.mean(energies)\n",
    "                    varE = np.var(energies)\n",
    "\n",
    "                    # append the elements\n",
    "                    WU_meanEnergy.append(meanE)\n",
    "                    WU_varEnergy.append(varE)\n",
    "\n",
    "                    # compute free energy and its variance\n",
    "                    meanF = np.mean(energies + T*log_probs)\n",
    "                    varF = np.var(energies + T*log_probs)\n",
    "\n",
    "                    # Do gradient step\n",
    "                    sess.run(optstep,feed_dict={E_placeholder:energies,samples_placeholder:samples,learningrate_placeholder: self.lr, T_placeholder:T})\n",
    "\n",
    "                    if it%5==0:\n",
    "                        print('WARM UP PHASE')\n",
    "                        print('mean(E): {0}, mean(F): {1}, var(E): {2}, var(F): {3}, #samples {4}, #Training step {5}'.format(meanE,meanF,varE,varF,self.numsamples, it))\n",
    "                        print(\"Temperature: \", T)        \n",
    "\n",
    "                    if time_count%5 == 0 and time_count>0:\n",
    "                        print(\"Elapsed time is =\", time.time()-start, \" seconds\")\n",
    "                        print('\\n\\n')\n",
    "\n",
    "                    time_count += 1    \n",
    "            \n",
    "                # containers for annealing loop\n",
    "                meanEnergy=[]\n",
    "                varEnergy=[]\n",
    "                varFreeEnergy = []\n",
    "                meanFreeEnergy = []\n",
    "                temperatures = []\n",
    "                \n",
    "                # annealing loop\n",
    "                for it0 in range(self.num_anneal):\n",
    "\n",
    "                    # reduce the temperature\n",
    "                    T = self.T0*(1-it0/self.num_anneal)\n",
    "                    temperatures.append(T)\n",
    "                    \n",
    "                    # training loop\n",
    "                    for it1 in range(self.num_train):\n",
    "\n",
    "                        samples, log_probs = sess.run(samplesandprobs)\n",
    "\n",
    "                        energies = nsp_matrixelements(samples, self.J, offset)\n",
    "\n",
    "                        meanE = np.mean(energies)\n",
    "                        varE = np.var(energies)\n",
    "\n",
    "                        #adding elements to be saved\n",
    "                        meanEnergy.append(meanE)\n",
    "                        varEnergy.append(varE)\n",
    "\n",
    "                        meanF = np.mean(energies + T*log_probs)\n",
    "                        varF = np.var(energies + T*log_probs)\n",
    "\n",
    "                        meanFreeEnergy.append(meanF)\n",
    "                        varFreeEnergy.append(varF)\n",
    "\n",
    "                        if it1%5==0:\n",
    "                            print('ANNEALING PHASE')\n",
    "                            print('mean(E): {0}, mean(F): {1}, var(E): {2}, var(F): {3}, #samples {4}, #Training step {5}'.format(meanE,meanF,varE,varF,self.numsamples, it))\n",
    "                            print(\"Temperature: \", T)\n",
    "\n",
    "                        # Do gradient step\n",
    "                        sess.run(optstep,feed_dict={E_placeholder:energies,samples_placeholder:samples,learningrate_placeholder: self.lr, T_placeholder:T})\n",
    "\n",
    "                        if time_count%5 == 0 and time_count>0:\n",
    "                            print(\"Ellapsed time is =\", time.time()-start, \" seconds\")\n",
    "                            print('\\n\\n')\n",
    "\n",
    "                        time_count += 1\n",
    "\n",
    "                # when training is done, generate 500000 samples, 50000 at a time\n",
    "                samples_per_step = 50000\n",
    "                n_steps = 10\n",
    "                samplesandprobs_final = self.PRNN.sample(numsamples=samples_per_step, inputdim=2)\n",
    "\n",
    "                samples_final = np.ones((samples_per_step*n_steps, self.N), dtype=np.int32)\n",
    "                energies_final = np.ones((samples_per_step*n_steps))\n",
    "\n",
    "                for i in range(n_steps):\n",
    "                    samples_step, _ = sess.run(samplesandprobs_final)\n",
    "                    energies_step = nsp_matrixelements(samples_step, self.J, offset)\n",
    "                    samples_final[(i)*samples_per_step : (i+1)*samples_per_step] = samples_step\n",
    "                    energies_final[(i)*samples_per_step : (i+1)*samples_per_step] = energies_step\n",
    "                \n",
    "                print(\"500,000 samples generated after training\")\n",
    "                    \n",
    "        return energies_final, samples_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "### Running VCA on NSP configuration of 15 days and 7 nurses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qubo, offset = dwave_nsp_qubo(n_days=15, n_nurses=7) \n",
    "N = qubo.shape[0]      # system size\n",
    "# set VCA hyperparameters\n",
    "\"\"\"\n",
    "n_warmup - number of training steps at the initial temperature T=T0\n",
    "n_anneal - duration of the annealing procedure\n",
    "n_train - number of training step during backprop after every annealing step\n",
    "dilated_layers - number of dilated layers for VCA-Dilated. Note: for other architectures, layers = 1\n",
    "\"\"\"\n",
    "n_warmup = 2000\n",
    "n_anneal = 128\n",
    "n_train = 5\n",
    "dilated_layers = np.int32(np.ceil(np.log2(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note:\n",
    "RNNtype specifies the RNN architecture among {'ws', 'nws', 'dilated'}\n",
    "rnn_unit specifies the RNN cell type among {'basic', 'lstm', 'gru'}\n",
    "\"\"\"\n",
    "# VCA-Dilated\n",
    "model = vca(N=N, n_layers=dilated_layers, n_warmup=n_warmup, n_anneal=n_anneal, n_train=n_train, J=qubo, offset=offset, RNNtype='dilated', rnn_unit='basic', T0=2)\n",
    "energies_dilated, samples_dilated = model.run()\n",
    "np.save('energies_vca-dilated'.format(n_anneal), energies_dilated)\n",
    "np.save('samples_vca-dilated'.format(n_anneal), samples_dilated)\n",
    "\n",
    "# VCA-Vanilla\n",
    "model = vca(N=N, n_layers=1, n_warmup=n_warmup, n_anneal=n_anneal, n_train=n_train, J=qubo, offset=offset, RNNtype='nws', rnn_unit='basic', T0=2)\n",
    "energies_vanilla, samples_vanilla = model.run()\n",
    "np.save('energies_vca-vanilla'.format(n_anneal), energies_vanilla)\n",
    "np.save('samples_vca-vanilla'.format(n_anneal), samples_vanilla)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c17fe89368f3df3b6fd3fdfd57958e43f86c5fba66a270cb992506caf886a193"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ds': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
